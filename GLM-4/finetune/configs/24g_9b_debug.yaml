data_config:
  train_file: index.txt
  val_file: standard.jsonl
  test_file: standard.jsonl
  num_proc: 1

combine: True
freezeV: True
max_input_length: 4000
max_output_length: 100
swanlab: "local"  # set to local if don`t use cloud

training_args:
  # see `transformers.Seq2SeqTrainingArguments`
  gradient_checkpointing: true
  # https://github.com/huggingface/transformers/blob/e8b292e35f331d3c3de85f7e5d3496b0e13d3d6f/src/transformers/training_args.py#L156
  optim: "adamw_torch_8bit"
  bf16: true
  output_dir: ./output
  num_train_epochs: 30
  # needed to be fit for the dataset
  learning_rate: 8e-5
  warmup_steps: 12
  # settings for data loading
  dataloader_drop_last: true
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 7
  dataloader_num_workers: 3
  remove_unused_columns: false
  # settings for saving checkpoints
  save_strategy: steps
  save_steps: 10
  # settings for logging
  log_level: info
  logging_strategy: steps
  logging_steps: 1
  run_name: "glm4-lora-finetune"
  report_to: "tensorboard"
  # settings for evaluation
  per_device_eval_batch_size: 1
  eval_strategy: steps
  eval_steps: 6
  # settings for optimizer
  # adam_epsilon: 1e-6
  # uncomment the following line to detect nan or inf values
  # debug: underflow_overflow
  predict_with_generate: true
  # see `transformers.GenerationConfig`
  generation_config:
    max_new_tokens: 100
  # set your absolute deepspeed path here
  # deepspeed: configs/ds_zero_3.json
  # deepspeed: configs/ds_zero_2.json

peft_config:
  peft_type: LORA
  task_type: CAUSAL_LM
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj"]
